syntax = "proto3";

enum TensorDataType {
    FLOAT32 = 0;
    FLOAT64 = 1;
    INT32 = 2;
    INT64 = 3;
    UINT8 = 4;
}

enum ModelStatus {
    UNINITIALIZED = 0;
    LOADING = 1;
    READY = 2;
    INFERRING = 3;
    ERROR = 4;
}

message TensorShape {
    option (proto2ffi.simd) = true;

    repeated uint32 dimensions = 1 [(proto2ffi.max_count) = 8];
    uint32 rank = 2;
    uint32 total_elements = 3;
}

message Tensor {
    option (proto2ffi.pool_size) = 1000;

    string name = 1 [(proto2ffi.max_length) = 64];
    TensorShape shape = 2;
    TensorDataType dtype = 3;
    repeated float data_f32 = 4 [(proto2ffi.max_count) = 1000000];
    uint32 data_count = 5;
}

message ModelParameters {
    option (proto2ffi.pool_size) = 100;

    string model_name = 1 [(proto2ffi.max_length) = 128];
    uint32 parameter_count = 2;
    repeated Tensor layers = 3 [(proto2ffi.max_count) = 100];
    uint32 layer_count = 4;
}

message InferenceRequest {
    option (proto2ffi.pool_size) = 5000;

    uint64 request_id = 1;
    uint64 timestamp_ns = 2;
    Tensor input = 3;
    uint32 batch_size = 4;
}

message InferenceResult {
    option (proto2ffi.pool_size) = 5000;

    uint64 request_id = 1;
    Tensor output = 2;
    uint64 inference_time_ns = 3;
    float confidence = 4;
}

message TrainingMetrics {
    option (proto2ffi.simd) = true;

    uint64 epoch = 1;
    uint64 iteration = 2;
    float loss = 3;
    float accuracy = 4;
    float learning_rate = 5;
    uint64 batch_time_ns = 6;
}

message BatchData {
    option (proto2ffi.pool_size) = 500;

    repeated Tensor inputs = 1 [(proto2ffi.max_count) = 64];
    repeated Tensor labels = 2 [(proto2ffi.max_count) = 64];
    uint32 batch_size = 3;
}

message PipelineStats {
    option (proto2ffi.simd) = true;

    uint64 total_inferences = 1;
    uint64 successful_inferences = 2;
    uint64 failed_inferences = 3;
    float average_latency_ms = 4;
    float throughput_per_second = 5;
    uint64 bytes_processed = 6;
}
